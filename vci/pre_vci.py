import argparse
import os
import glob
import shutil
import json
from PIL import Image, ImageChops
from tqdm import tqdm

import numpy as np
from skimage import filters, transform
import skimage as ski
from scipy import ndimage

# dont question this ... :/
from pathlib import Path
import sys
path_root = Path(__file__).parents[1]
sys.path.append(str(path_root))

#from script.thirdparty.colmap_database_3_9 import *
from script.thirdparty.colmap_database_3_10 import *
from script.downsample_point import process_ply_file
from vci.pose_utils import *
from vci.sys_utils import *

# Helper functions
def load_paths(args: argparse.Namespace) -> dict[str, str]:
	paths: dict[str, str] = {}

	paths['base'] = args.source_path
	paths['calibration'] = os.path.join(paths['base'], args.calibration_file) # Will discard paths['base'] if args.calibration_file is an absolute path

	paths['colmap'] = os.path.join(paths['base'], "colmap")

	paths['masks'] = os.path.join(paths['colmap'], "masks")
	paths['input'] = os.path.join(paths['colmap'], "input")
	paths['distorted'] = os.path.join(paths['colmap'], "distorted")
	paths['manual'] = os.path.join(paths['colmap'], "manual")

	paths['imagestxt'] = os.path.join(paths['manual'], "images.txt")
	paths['camerastxt'] = os.path.join(paths['manual'], "cameras.txt")
	paths['points3Dtxt'] = os.path.join(paths['manual'], "points3D.txt")

	if args.gaussian_splatting:
		paths['db'] = os.path.join(paths['distorted'], "database.db")

		paths['sparse'] = os.path.join(paths['distorted'], "sparse", "0")
		paths['dense'] = paths['colmap']
	else:
		paths['db'] = os.path.join(paths['colmap'], "database.db")
		
		paths['sparse'] = os.path.join(paths['distorted'], "sparse")
		paths['dense'] = os.path.join(paths['colmap'], "dense", "workspace")

	paths['output'] = os.path.join(paths['dense'], "fused.ply")
	paths['undistorted_images'] = os.path.join(paths['dense'], "images")

	return paths

def cam_name(idx: int) -> str:
	return "cam" + str(idx).zfill(2)

def calculate_masks(frames, bgs):
	diff = np.abs(frames - bgs)
	diff = np.mean(diff, axis=-1)

	#thres = ndimage.maximum_filter(diff, size=(3,3), axes=(1,2))
	diff = ndimage.gaussian_filter(diff, sigma=5, axes=(-1, -2))

	return np.where(diff < 0.05, 0.0, 1.0)

def copy_images(paths: dict[str, str], args: argparse.Namespace) -> None:
	frames = sorted([f for f in os.listdir(paths['base']) if f.startswith("frame")])
	cameras = sorted([f for f in os.listdir(os.path.join(paths['base'], frames[0], "rgb"))])

	num_frames = len(frames)
	num_cams = len(cameras)
	num_imgs = num_frames * num_cams

	print(f"Number of frames: {num_frames}")
	print(f"Number of cameras: {num_cams}")
	print(f"Total number of images: {num_imgs}")
	print()

	# Read background images
	bg_path = os.path.join(paths['base'], "background", "rgb", "*")
	bgs = ski.io.imread_collection(bg_path, conserve_memory=False)

	# Create image dir
	if not create_dir(os.path.join(paths['base'], "images")):
		print("Input folder already exists")

		if not args.replace_images:
			return
		
		print("Images will be replaced. If this is undesired, use --replace_images")
	
	for j, cam in enumerate(cameras):
		create_dir(os.path.join(paths['base'], "images", cam_name(j)))

	# Copy images
	t = tqdm(desc="Copying images", total=num_imgs)
	for i, frame in enumerate(frames):
		for j, cam in enumerate(cameras):
			src = os.path.join(paths['base'], frame, "rgb", cam)
			dst = os.path.join(paths['base'], "images", cam_name(j), str(i).zfill(4) + ".jpg")

			shutil.copyfile(src, dst)

			t.update()
	t.close()

	# Mask them
	if not args.use_masks:
		return

	t = tqdm(desc="Applying masks", total=num_imgs)
	for j, cam in enumerate(cameras):
		bg_img = bgs[j] / 255.0

		cam_path = os.path.join(paths['base'], "images", cam_name(j))

		frames_imgs = ski.io.imread_collection(os.path.join(cam_path, "*"), conserve_memory=False)

		for i in range(len(frames)):
			t.set_postfix({	"Camera": os.path.splitext(cam)[0],
							"Image": i })

			frame = frames_imgs[i] / 255.0
			mask = calculate_masks(frame, bg_img)
			masked = frame * mask[..., None]

			ski.io.imsave(os.path.join(cam_path, str(i).zfill(4) + ".jpg"), np.uint8(masked * 255.0), check_contrast=False)

			t.update()
	t.close()


# Main functionality
def extract_images(paths: dict[str, str], args: argparse.Namespace) -> tuple[list[str], list[str]]:
	"""
	Extract the images and masks provided in paths['base'] and paths['mask_source'] respectively. The
	files will be copied into paths['input'] and paths['masks'] respectively. If --resolution is set
	the files will be downscaled accordingly and if --use_masks is set the masks will be applied.

	Params:
		paths (dict[str, str]):    A dictionary containing the filepaths
		args (argparse.Namespace): The command line arguments generated by argparse

	Returns:
		images (list[str]): A list of all input images found in paths['base']
		masks (list[str]):  A list of all masks found in paths['mask_source']
	"""

	valid_filetypes = [".png", ".jpg"]

	img_path = os.path.join(paths['base'], "frame_00000", "rgb")
	bg_path = os.path.join(paths['base'], "background", "rgb")

	images = sorted([file for file in os.listdir(img_path) if os.path.splitext(file)[1] in valid_filetypes])
	bgs = sorted([file for file in os.listdir(bg_path) if os.path.splitext(file)[1] in valid_filetypes])

	# images.remove("C0018.jpg")
	# masks.remove("C0018_mask.png")

	if len(images) == 0:
		print("Found no images. Skipping image extraction")
		return (images, bgs)
	
	copy_images = create_dir(paths['input'])
	copy_masks = create_dir(paths['masks']) and args.use_masks

	if not copy_images and not copy_masks:
		print("Image and mask directories already exist. Skipping image extraction (if this is undesired, use --replace_images)")
		return (images, bgs)

	downscale_factor = 1.0 / args.resolution

	for i, img_name in tqdm(enumerate(images), "Preparing input images", total=len(images)):
		img = ski.io.imread(os.path.join(img_path, img_name)) / 255.0
		bg = ski.io.imread(os.path.join(bg_path, img_name)) / 255.0

		if args.use_masks:
			mask = calculate_masks(img, bg)
			img *= mask[..., None]

			mask = transform.rescale(mask, downscale_factor, anti_aliasing=False)
			ski.io.imsave(os.path.join(paths['masks'], cam_name(i) + ".jpg.png"), np.uint8(mask * 255.0), check_contrast=False)

		img = transform.rescale(img, downscale_factor, anti_aliasing=False)
		ski.io.imsave(os.path.join(paths['input'], cam_name(i) + ".jpg"), np.uint8(img * 255.0), check_contrast=False)

	return images, bgs

def extract_poses(filenames: list[str], paths: dict[str, str], args: argparse.Namespace) -> None:
	"""
	Extracts camera pose information from paths['calibration']. The poses will be written 
	to the sqlite database located at paths['db'] and to the .txt files located in the directory
	paths['manual']. The generated files can be processed directly by COLMAP.
	
	Params:
		filenames (list[str]):     A list of the filenames of the image sources as returned by extract_images
		paths (dict[str, str]):    A dictionary containing the filepaths
		args (argparse.Namespace): The command line arguments generated by argparse

	Returns:
		None
	"""

	# Set camera model
	camera_models = {"SIMPLE_PINHOLE": 0, "PINHOLE": 1 , "OPENCV": 4 }
	if not args.camera in camera_models.keys():
		print(f"Unsupported camera model: {args.camera}")
		exit()

	camera_model = camera_models[args.camera]
	
	# Downscale factor
	s = 1.0 / args.resolution

	# Load calibration file
	assert os.path.exists(paths['calibration'])
	calibration_file = open(paths['calibration'])
	calibration = json.load(calibration_file)
	calibration_file.close()

	# Create necessary directories
	create_dir(paths['manual'])
	db_dir = os.path.dirname(paths['db'])
	create_dir(db_dir)

	imagetxt_list = []
	cameratxt_list = []

	db = COLMAPDatabase.connect(paths['db'])
	db.create_tables()

	print(f"Start writing to new database at '{paths['db']}'")

	avg_P = np.average(np.column_stack([-V[:3, :3].T @ V[:3, 3] for V in [np.array(cam["extrinsics"]["view_matrix"]).reshape((4, 4)) for cam in calibration["cameras"]]]), axis=1)
	print(avg_P)

	cam_idx = 0

	for i, camera in tqdm(enumerate(calibration["cameras"]), desc="Reading camera calibration", total=len(calibration['cameras'])):
		# Find corresponding images
		camera_name = camera["camera_id"]
		matching_images = [f for f in filenames if camera_name in f]

		if len(matching_images) == 0:
			print(f"Missing image source for camera {camera_name}. Skipping this camera.")
			continue

		image_name = "cam" + str(cam_idx).zfill(2) + ".jpg"
		cam_idx += 1

		img = Image.open(os.path.join(paths['input'], image_name))
		width, height = img.size

		# Extract pose information from the JSON file
		view_matrix = np.array(camera["extrinsics"]["view_matrix"]).reshape((4, 4)) # View Matrix is given in World-To-Camera Space (i think)
		view_matrix = rotate_z_view_matrix(view_matrix)

		camera_matrix = np.array(camera["intrinsics"]["camera_matrix"]).reshape((3, 3))

		distortion_coefficients = np.array(camera["intrinsics"]["distortion_coefficients"])

		# Camera rotation and translation
		R = view_matrix[:3, :3]
		T = view_matrix[:3, 3]
		Q = rotation_matrix_to_quaternion(R)

		P = -R.T @ T
		P -= avg_P
		T = -R @ P

		# focal length
		f_x = camera_matrix[0, 0]
		f_y = camera_matrix[1, 1]

		# principal point
		c_x = camera_matrix[0, 2]
		c_y = camera_matrix[1, 2]
		
		# Correct the parameters if a camera has incorrect values for some reason...
		json_width = int(camera["intrinsics"]["resolution"][0] * s)
		json_height = int(camera["intrinsics"]["resolution"][1] * s)

		if json_width != width or json_height != height:
			x_correction = float(width) / float(json_width)
			y_correction = float(height) / float(json_height)

			f_x *= x_correction
			f_y *= y_correction
			c_x *= x_correction
			c_y *= y_correction

		# Downscale
		f_x *= s
		f_y *= s

		c_x *= s
		c_y *= s

		# Write camera and image data into database
		if args.camera == "SIMPLE_PINHOLE":
			params = np.array([f_x, c_x, c_y])
		elif args.camera == "PINHOLE":
			params = np.array([f_x, f_y, c_x, c_y])
		elif args.camera == "OPENCV":
			params = np.array([f_x, f_y, c_x, c_y, distortion_coefficients[0], distortion_coefficients[1], distortion_coefficients[2], distortion_coefficients[3]])

		camera_id = db.add_camera(camera_model, width, height, params)

		#image_id = db.add_image(image_name, camera_id, Q, T, image_id=i+1) # For COLMAP <= 3.9
		image_id = db.add_image(image_name, camera_id, image_id=i+1) # For COLMAP >= 3.10

		#db.add_pose_prior(image_id, P)

		db.commit()

		# Append lines for images.txt and cameras.txt
		Q_string = " ".join([str(q) for q in Q])
		T_string = " ".join([str(t) for t in T])
		params_string = " ".join([str(num) for num in params])

		image_line = f"{image_id} {Q_string} {T_string} {camera_id} {image_name}\n"
		imagetxt_list.append(image_line)
		imagetxt_list.append("\n")

		camera_line = f"{camera_id} {args.camera} {width} {height} {params_string}\n"
		cameratxt_list.append(camera_line)

	db.close()

	print("Done writing to database")
	
	# Write prepared data into images.txt and cameras.txt
	write_lines_to_file(imagetxt_list, paths['imagestxt'])
	write_lines_to_file(cameratxt_list, paths['camerastxt'])
	write_lines_to_file([], paths['points3Dtxt'])

	print("Done writing text output")

def run_colmap(image_names: list[str], paths: dict[str, str], args: argparse.Namespace) -> None:
	"""
	Executes the COLMAP command, producing a sparse or dense reconstruction of the
	input images.
	
	Params:
		image_names (list[str]):   A list of the filenames of the image sources as returned by extract_images
		paths (dict[str, str]):    A dictionary containg the filepaths
		args (argparse.Namespace): The command line arguments generated by argparse

	Returns:
		None
	"""

	# Create necessary directories
	create_dir(paths['sparse'])
	create_dir(paths['dense'])
	
	# Colmap commands
	print("Starting COLMAP...")
	print()

	feature_extract = f"colmap feature_extractor \
		--database_path {paths['db']} \
		--image_path {paths['input']} \
		--SiftExtraction.estimate_affine_shape=true \
		--SiftExtraction.domain_size_pooling=true "
	# Pass masks to colmap so there are no points generated for the background
	if args.use_masks:
		feature_extract += f" --ImageReader.mask_path {paths['masks']}"
	exec_cmd(feature_extract)

	feature_matcher = f"colmap exhaustive_matcher \
		--database_path {paths['db']} \
		--SiftMatching.guided_matching=true" # --TwoViewGeometry.min_num_inliers 5
	exec_cmd(feature_matcher)

	tri_and_map = f"colmap point_triangulator \
		--database_path {paths['db']} \
		--image_path {paths['input']} \
		--input_path {paths['manual']} \
		--output_path {paths['sparse']} \
		--Mapper.ba_global_function_tolerance=0.000001 \
		--Mapper.ba_refine_focal_length=0 \
		--Mapper.ba_refine_principal_point=0 \
		--Mapper.ba_refine_extra_params=0 \
		--refine_intrinsics=0" # --Mapper.min_num_matches 5 --Mapper.init_min_num_inliers 40 --Mapper.ba_global_function_tolerance=0.000001
	exec_cmd(tri_and_map)

	image_undistortion = f"colmap image_undistorter \
		--image_path {paths['input']} \
		--input_path {paths['sparse']} \
		--output_path {paths['dense']}"
	exec_cmd(image_undistortion)

	if not args.skip_dense:
		patch_match_stereo = f"colmap patch_match_stereo \
			--workspace_path {paths['dense']}"
		exec_cmd(patch_match_stereo)

		stereo_fusion = f"colmap stereo_fusion \
			--workspace_path {paths['dense']} \
			--output_path {paths['output']} \
			--output_type PLY" # --StereoFusion.mask_path {paths['masks']}
		exec_cmd(stereo_fusion)

		# Generate binary files
		stereo_fusion_bin = f"colmap stereo_fusion \
			--workspace_path {paths['dense']} \
			--output_path {os.path.dirname(paths['output'])} \
			--output_type BIN" # --StereoFusion.mask_path {paths['masks']}
		exec_cmd(stereo_fusion_bin)

		print(f"All done! The output is in '{paths['output']}'")
	else:
		sparse = os.path.join(paths['dense'], "sparse")
		sparse0 = os.path.join(paths['dense'], "sparse", "0")

		files = os.listdir(sparse)
		create_dir(sparse0)

		# Copy each file from the source directory to the destination directory (required by 3DGS)
		for file in tqdm(files, desc=f"Moving output files", total=len(files)):
			if file == '0':
				continue

			source_file = os.path.join(sparse, file)
			destination_file = os.path.join(sparse0, file)
			shutil.move(source_file, destination_file)

	print("Done.")


def main():
	parser = argparse.ArgumentParser(prog="python pre_vci.py", description="Generates a sparse/dense point cloud from a set of input images and camera poses generated by the VCI")
	parser.add_argument("--source_path", "-s", default="", type=str, required=True, help="the path where the image files are located")
	parser.add_argument("--mask_source", default="", type=str, help="the path to the image masks (default: source_path)")
	parser.add_argument("--calibration_file", "-c", default="calibration.json", type=str, help="the path to the camera calibration file (default: %(default)s)")
	parser.add_argument("--resolution", "-r", default=1, type=int, choices=[1,2,4,8], help="downscale the image by factor r (default: %(default)s)")
	parser.add_argument("--camera", default="PINHOLE", type=str, choices=["SIMPLE_PINHOLE", "PINHOLE", "OPENCV"], help="the camera model used when extracting the poses (default: %(default)s)")
	parser.add_argument("--replace_images", action="store_true", default=False, help="copies the images and masks into the input folder, replacing the old ones if necessary (default: %(default)s)")
	parser.add_argument("--gaussian_splatting", action="store_true", default=False, help="enables output for 3d gaussian splatting (default: %(default)s)")
	parser.add_argument("--skip_dense", action="store_true", default=False, help="skips dense reconstruction (True when --gaussian_splatting is set, default: %(default)s)")
	parser.add_argument("--use_masks", action="store_true", default=False, help="enables usage of masks in the feature extractor (default: %(default)s)")
	args = parser.parse_args()

	if args.gaussian_splatting:
		args.skip_dense = True

	# Load all the paths from the passed arguments
	paths = load_paths(args)
	assert os.path.exists(paths['base'])

	# Print out the paths
	print("Set directories:")
	for k, v in paths.items():
		print(f"{k:<20} -> {v}")
	print()

	# Remove old files and directories
	if os.path.exists(paths['colmap']):
		whitelist: list[str] = []
		if not args.replace_images:
			whitelist = [paths['input'], paths['masks']]

		removed = clear_directory(paths['colmap'], whitelist)
		print("Removed files and directories:")
		print("\n".join(removed))
		print()

	print(f"Preparing data from '{paths['base']}'")
	print()

	copy_images(paths, args)

	images, masks = extract_images(paths, args)

	extract_poses(images, paths, args)

	run_colmap(images, paths, args)

	process_ply_file(paths['output'], os.path.join(paths['base'], "points3D_downsample.ply"))

if __name__ == "__main__":
	main()